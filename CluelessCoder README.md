## Speaker Diarization with Whisper & NeMo â€“ CluelessCoder Edition

  

[![Build](https://img.shields.io/github/actions/workflow/status/MahmoudAshraf97/whisper-diarization/test_run.yml?branch=main)](https://github.com/MahmoudAshraf97/whisper-diarization/actions)

[![License: MIT](https://img.shields.io/badge/license-MIT-yellow)](LICENSE)

![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)

![Runs on](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MahmoudAshraf97/whisper-diarization/blob/main/Whisper_Transcription_%2B_NeMo_Diarization.ipynb)

  

---

  

## 1Â· What & Why

  

This project combines OpenAI Whisper ASR with NeMo diarization to label speakers in audio. **Before following the install instructions below, it is highly advisable to first work with [`Whisper_Transcription_+_NeMo_Diarization.ipynb`](Whisper_Transcription_+_NeMo_Diarization.ipynb) in Colab or locally. 

If you work locally be aware that there are different requirements for GPU `requirements.txt` and iGPU/CPU workloads `requirements-no-torch.txt`.**

**For your first run drop your mp3 audio into the root folder and work through the Jupyter notebook which is setup to look for audio in the root folder.**

  

> **Why?** The notebook lets you experiment interactively, helps you resolve hardware issues (e.g., CUDA, GPU/iGPU/CPU, RAM), and ensures you can build a working script before running the full pipeline. Once you have a working notebook, you can convert it into a script for on-demand use.

  

The install steps in the original [README.md](README.md) are correct, but the notebook-first approach will save you time and frustration, especially if youâ€™re new to ML or have a non-standard setup.

  

---

  

## 2Â· Techâ€‘Stack Overview

  

| Layer Â  Â  Â  Â  | Tech Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

| -------------| ------------------------------------------ |

| **Language** | Python 3.10+ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |

| **ASR** Â  Â  Â | OpenAI Whisper, Faster-Whisper Â  Â  Â  Â  Â  Â  |

| **Diarization** | NVIDIA NeMo, TitaNet, MarbleNet Â  Â  Â  Â  |

| **Separation** | Demucs (optional, for music removal) Â  Â  Â |

| **Alignment** | ctc-forced-aligner Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

| **Punctuation** | DeepMultilingualPunctuation Â  Â  Â  Â  Â  Â  Â |

| **Notebook** | Jupyter/Colab Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

  

---

  

## 3Â· Key Features

  

â€¢ Speaker-labeled transcription from audio

â€¢ Handles music/speech separation (optional)

â€¢ Forced alignment for accurate timestamps

â€¢ Punctuation restoration for readable output

â€¢ Beginner-friendly notebook for rapid prototyping

  

---

  

## 4Â· System Overview Diagram

  

```mermaid

flowchart TD

Â  Â  A[Audio File] --> B["Demucs (optional)"]

Â  Â  B --> C[Whisper ASR]

Â  Â  C --> D[Forced Alignment]

Â  Â  D --> E[NeMo Diarization]

Â  Â  E --> F[Punctuation Model]

Â  Â  F --> G[Speaker-Labeled Transcript]

Â  Â  style A fill:#cceeff,stroke:#333333,color:#111111

Â  Â  style B fill:#ffddcc,stroke:#333333,color:#111111

Â  Â  style C fill:#bbf7d0,stroke:#333333,color:#111111

Â  Â  style D fill:#fffacd,stroke:#333333,color:#111111

Â  Â  style E fill:#eeeeee,stroke:#333333,color:#111111

Â  Â  style F fill:#cceeff,stroke:#333333,color:#111111

Â  Â  style G fill:#bbf7d0,stroke:#333333,color:#111111

```

  

---

  

## 5Â· Folder / File Guide

  

```text

ğŸ“‚ repo-root

Â â”œâ”€ diarize.py Â  Â  Â  Â  Â  Â  Â  Â # Main script (after notebook)

Â â”œâ”€ Whisper_Transcription_+_NeMo_Diarization.ipynb Â # Recommended starting point

Â â”œâ”€ requirements.txt Â  Â  Â  Â  Â # Python dependencies

Â â”œâ”€ constraints.txt Â  Â  Â  Â  Â  # Dependency constraints

Â â”œâ”€ README.md Â  Â  Â  Â  Â  Â  Â  Â  # Original instructions

Â â”œâ”€ .gitignore

```

  

```mermaid

flowchart LR

Â  Â  code["diarize.py"]

Â  Â  nb["Whisper_Transcription_+_NeMo_Diarization.ipynb"]

Â  Â  reqs["requirements.txt"]

Â  Â  constr["constraints.txt"]

Â  Â  style code fill:#cceeff,stroke:#333333,color:#111111

Â  Â  style nb fill:#bbf7d0,stroke:#333333,color:#111111

Â  Â  style reqs fill:#fffacd,stroke:#333333,color:#111111

Â  Â  style constr fill:#eeeeee,stroke:#333333,color:#111111

```

  

---

  

## 6Â· Prerequisites & Accounts

  

| Need this Â  Â  Â  Â  | Why Â  Â  Â  Â  Â  Â  Â  Â | Link Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

| ---------------- | ------------------ | ------------------------------------- |

| Python 3.10+ Â  Â  Â | core runtime Â  Â  Â  | https://python.org Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

| FFMPEG, Cython Â  Â | audio processing Â  | see install steps below Â  Â  Â  Â  Â  Â  Â  |

| Jupyter/Colab Â  Â  | run notebook Â  Â  Â  | https://colab.research.google.com Â  Â  |

| CUDA Toolkit Â  Â  Â | GPU acceleration Â  | https://developer.nvidia.com/cuda-downloads |

  

---

  

## 7Â· Setup Options (Recommended: Notebook First)

  

###Â A.Â Notebook (Best for Beginners)

  

1. Open [`Whisper_Transcription_+_NeMo_Diarization.ipynb`](Whisper_Transcription_+_NeMo_Diarization.ipynb) in Colab or Jupyter.

2. Run cells top-to-bottom, following prompts for dependencies.

3. Tweak parameters (audio file, model, device, etc) as needed.

4. Once working, export as script or adapt to `diarize.py`.

  

###Â B.Â Script (After Notebook Success)

  

Follow the original [README.md](README.md) install steps:

  

```bash

pip install cython

sudo apt install ffmpeg

pip install -c constraints.txt -r requirements.txt

python diarize.py -a AUDIO_FILE_NAME

```

  

---

  

## 8Â· Setup Flowchart

  

```mermaid

flowchart TD

Â  Â  A[Clone Repo] --> B{Choose Setup}

Â  Â  B -->|Notebook| C[Run Notebook]

Â  Â  C --> D[Export/Adapt Script]

Â  Â  D --> E[Run diarize.py]

Â  Â  B -->|Script| E

Â  Â  style B fill:#fffacd,stroke:#333333,color:#111111

Â  Â  style C fill:#cceeff,stroke:#333333,color:#111111

Â  Â  style D fill:#ffddcc,stroke:#333333,color:#111111

Â  Â  style E fill:#bbf7d0,stroke:#333333,color:#111111

```

  

---

  

## 9Â· Run / Test

  

```bash

# Run notebook interactively

jupyter notebook Whisper_Transcription_+_NeMo_Diarization.ipynb

  

# Or run script after setup

python diarize.py -a AUDIO_FILE_NAME

  

# Run all unit tests (if available)

pytest -q

```

  

---

  

## 10Â· Configuration & API Keys ğŸ”‘

  

- No API keys required for local use.

- For Colab, ensure you have access to a GPU (Runtime > Change runtime type > GPU).

- If using cloud APIs, add keys to `.env` (see original README).

  

> âš ï¸Â Do **not** commit `.env` files. Use repository secrets for Codespaces or CI.

  

---

  

## 11Â· Troubleshooting / FAQ

  

| Symptom Â  Â  Â  Â  Â  Â  Â  Â  | Cause Â  Â  Â  Â  Â  Â  Â | Fix Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |

| ----------------------- | ------------------ | ------------------------------------- |

| `ModuleNotFoundError` Â  | venv not activated | `source .venv/bin/activate` Â  Â  Â  Â  Â  |

| VSÂ Code â€œcannot attachâ€ | Docker daemon off Â | Start Docker Desktop / Podman Â  Â  Â  Â  |

| CUDA not found Â  Â  Â  Â  Â | No GPU/driver Â  Â  Â | Use CPU or check Colab runtime Â  Â  Â  Â |

  

---

  

## 12Â· Status & Roadmap

  

âœ… Whisper + NeMo pipeline

âœ… Notebook-first workflow

â³ CLI improvements

ğŸ”œ Overlapping speaker support

âš ï¸ Known: Hardware/driver issues may require notebook debugging

  

---

  

## 13Â· How AI Helped

  

ChatGPT and Copilot assisted with code, docs, and troubleshooting steps.

  

---

  

## 14Â· License

  

MIT â€“ see [LICENSE](LICENSE).

  

---

  

## 15Â· Community

  

â€¢ Open an issue for questions/ideas

â€¢ PRs welcome â€“ even docs or tests!

â€¢ New to coding? Tag your issue with **`beginnerâ€‘help`** and weâ€™ll mentor you.

  

---

  

*Happy hacking â€“ may your tokens be ever cheaper!* ğŸ‰